{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a PyTorch implementation of Long Short-Term Memory.\n",
    "    c for long-term memory\n",
    "    h for short-term memory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, apply_layer_norm, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.apply_layer_norm = apply_layer_norm\n",
    "\n",
    "        self.W_x = nn.Linear(input_dim, 4*hidden_dim, bias = False)\n",
    "        self.W_h = nn.Linear(hidden_dim, 4*hidden_dim)\n",
    "\n",
    "        if apply_layer_norm:\n",
    "            self.layer_norm = nn.ModuleList([\n",
    "                nn.LayerNorm(hidden_dim)\n",
    "                for _ in range(4)\n",
    "            ])\n",
    "            self.layer_norm_c = nn.LayerNorm(hidden_dim)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        x: batch_size, input_dim\n",
    "        h_prev: batch_size, hidden_dim\n",
    "        c_prev: batch_size, hidden_dim\n",
    "        \"\"\"\n",
    "        gates = self.W_x(x) + self.W_h(h_prev) # i, f, g, o\n",
    "        gates = gates.chunk(4, dim = -1)\n",
    "\n",
    "        if self.apply_layer_norm:\n",
    "            gates = [self.layer_norm[i](gates[i]) for i in range(4)]\n",
    "\n",
    "        i, f, g, o = gates\n",
    "\n",
    "        c_updated = F.sigmoid(f) * c_prev + F.sigmoid(i) * F.tanh(g)\n",
    "        h_updated = F.sigmoid(o) * F.tanh(self.layer_norm_c(c_updated) \\\n",
    "            if self.apply_layer_norm else c_updated)\n",
    "\n",
    "        return h_updated, c_updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ScrachLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, n_layers: int, return_sequences: bool = True, batch_first = True) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.return_sequences = return_sequences\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.cells = nn.ModuleList(\n",
    "            [LSTMCell(input_dim, hidden_dim, apply_layer_norm = False)] + \n",
    "            [LSTMCell(hidden_dim, hidden_dim, apply_layer_norm = False)\n",
    "            for _ in range(n_layers - 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: \n",
    "        x has shape (batch_size, n_steps, feature_dim) if batch_first \n",
    "        else (n_steps, batch_size, feature_dim)\n",
    "        and state is a tuple of h and c, each with a shape of (batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "            #print(x.shape)\n",
    "        n_steps, batch_size = x.shape[:2]\n",
    "        \n",
    "        h = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        c = torch.zeros_like(h)\n",
    "\n",
    "        out = []\n",
    "        for t in range(n_steps):\n",
    "\n",
    "            inputs = x[t]\n",
    "\n",
    "            for l in range(self.n_layers):\n",
    "                h[l], c[l] = self.cells[l](inputs, h[l], c[l])\n",
    "                inputs = h[l]\n",
    "\n",
    "            out.append(h[-1])\n",
    "\n",
    "        if not self.return_sequences: out = [out[-1]]\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, n_steps, feature_dim = 2, 10, 54\n",
    "hidden_dim = 27\n",
    "\n",
    "x = torch.rand(batch_size, n_steps, feature_dim)\n",
    "\n",
    "myLSTM = ScrachLSTM(feature_dim, hidden_dim, 3, return_sequences=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, (h,c) = myLSTM(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de4552d2adf5fffb15137fbd10619e3ca728a7b0d5298f8ca4e80460cb991e07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
