{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PrepareForMHA(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    This module does a linear transformation and splits the vector into given number of heads for multi-head attention.\n",
    "    This is used to transform key, query, and value vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_heads: int,\n",
    "                 embed_dim: int,\n",
    "                 key_dim: int,\n",
    "                 bias: bool\n",
    "                 ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Input has shape [seq_len, batch_size, d_model] or [batch_size, d_model].\n",
    "        We apply the linear transformation to the last dimension and split that into the heads.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, num_heads * key_dim, bias = bias)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        head_shape = x.shape[:-1]\n",
    "\n",
    "        \"\"\"\n",
    "        split last dimension into heads\n",
    "        output has shape [seq_len, batch_size, heads, d_k] or [batch_size, d_model]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.fc(x).reshape(*head_shape, self.num_heads, self.key_dim)\n",
    "\n",
    "class MHA(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_heads: int,\n",
    "                 embed_dim: int,\n",
    "                 dropout_p: float,\n",
    "                 bias: bool = True\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = embed_dim // num_heads\n",
    "\n",
    "        self.W_Q = PrepareForMHA(num_heads, embed_dim, self.key_dim, bias = bias)\n",
    "        self.W_K = PrepareForMHA(num_heads, embed_dim, self.key_dim, bias = bias)\n",
    "        self.W_V = PrepareForMHA(num_heads, embed_dim, self.key_dim, bias = True)\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.scale = 1 / torch.sqrt(self.key_dim)\n",
    "\n",
    "        self.attn = None\n",
    "\n",
    "    def forward(self,\n",
    "                query: torch.Tensor,\n",
    "                key: torch.Tensor,\n",
    "                value: torch.Tensor,\n",
    "                attn_mask: Optional[torch.Tensor] = None,\n",
    "                need_weights: bool = True):\n",
    "\n",
    "        seq_len, batch_size, _ = query.shape\n",
    "\n",
    "        # Prepare query, key and value for attention computation.\n",
    "        # These will then have shape [seq_len, batch_size, heads, key_dim].\n",
    "        Q = self.W_Q(query)\n",
    "        K = self.W_K(key)\n",
    "        V = self.W_V(value)\n",
    "\n",
    "        scores = self.scale * torch.einsum('ibhd,jbhd->ijbh', query, key) # or  Q * K^âŠ¤\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            mask = self.prepare_mask(mask, query.shape, key.shape)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        scores = self.dropout(F.softmax(scores))\n",
    "\n",
    "        output = torch.einsum(\"ijbh,jbhd -> ibhd\", scores, V).reshape(seq_len, batch_size, -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return (output, scores) if need_weights else output\n",
    "\n",
    "    \n",
    "    def prepare_mask(self,\n",
    "                     mask: torch.Tensor,\n",
    "                     query_shape: List[int],\n",
    "                     key_shape: List[int]):\n",
    "\n",
    "        \"\"\"\n",
    "        mask has shape [seq_len_q, seq_len_k, batch_size],\n",
    "        where first dimension is the query dimension.\n",
    "        If the query dimension is equal to 1 it will be broadcasted.\n",
    "        \"\"\"\n",
    "\n",
    "        assert \\\n",
    "            mask.shape[0] == 1 or mask.shape[0] == query_shape[0] \\\n",
    "            or mask.shape[1] == key_shape[0] \\\n",
    "            or mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n",
    "\n",
    "        return mask.unsqueeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d75bf56f6470173b1ac7bd9da905d241387559ca422a37986719b723ab6f9d4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
