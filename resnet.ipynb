{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ShortcutProjection(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 stride: int,\n",
    "                 apply_bn = True) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
    "                              kernel_size=1, stride=stride)\n",
    "        self.bn = nn.BatchNorm2d(out_channels) \\\n",
    "            if apply_bn \\\n",
    "            else nn.Identity()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.bn(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 n_convs: int = 2,\n",
    "                 stride: int = 1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        dims = zip(\n",
    "            [in_channels] + [out_channels] * (n_convs - 1),\n",
    "            [out_channels] * n_convs)\n",
    "\n",
    "        self.backbone = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                OrderedDict([\n",
    "                    (f\"conv_{i}\", nn.Conv2d(ch_in, ch_out, 3, stride = stride, padding=1)),\n",
    "                    (f\"bn_{i}\", nn.BatchNorm2d(ch_out))])\n",
    "                )\n",
    "            for i, (ch_in, ch_out) in enumerate(dims, start = 1)])\n",
    "\n",
    "        self.shortcut = ShortcutProjection(in_channels, out_channels, stride) \\\n",
    "            if stride != 1 or in_channels != out_channels \\\n",
    "            else nn.Identity()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        f_x = x\n",
    "        for conv_bn in self.backbone[:-1]:\n",
    "            #print(i)\n",
    "            f_x = F.relu(conv_bn(f_x))\n",
    "        \n",
    "        return F.relu(\n",
    "            self.backbone[-1](f_x)) + self.shortcut(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "1 Sequential(\n",
      "  (conv_1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "2 Sequential(\n",
      "  (conv_2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "3 Sequential(\n",
      "  (conv_3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_3): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "4 Sequential(\n",
      "  (conv_4): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_4): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "5 Sequential(\n",
      "  (conv_5): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_5): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "6 Sequential(\n",
      "  (conv_6): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_6): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "7 Sequential(\n",
      "  (conv_7): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_7): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "8 Sequential(\n",
      "  (conv_8): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_8): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "9 Sequential(\n",
      "  (conv_9): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_9): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "10 Sequential(\n",
      "  (conv_10): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_10): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "11 Sequential(\n",
      "  (conv_11): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_11): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "12 Sequential(\n",
      "  (conv_12): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_12): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "13 Sequential(\n",
      "  (conv_13): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_13): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "14 Sequential(\n",
      "  (conv_14): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_14): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "15 Sequential(\n",
      "  (conv_15): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_15): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "16 Sequential(\n",
      "  (conv_16): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_16): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "17 Sequential(\n",
      "  (conv_17): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_17): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "18 Sequential(\n",
      "  (conv_18): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_18): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "19 Sequential(\n",
      "  (conv_19): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_19): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "20 Sequential(\n",
      "  (conv_20): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_20): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "21 Sequential(\n",
      "  (conv_21): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_21): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "22 Sequential(\n",
      "  (conv_22): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_22): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "23 Sequential(\n",
      "  (conv_23): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_23): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "24 Sequential(\n",
      "  (conv_24): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_24): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "25 Sequential(\n",
      "  (conv_25): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_25): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "26 Sequential(\n",
      "  (conv_26): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_26): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "27 Sequential(\n",
      "  (conv_27): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_27): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "28 Sequential(\n",
      "  (conv_28): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_28): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "29 Sequential(\n",
      "  (conv_29): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn_29): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resblock = ResidualBlock(3,5, n_convs=30)\n",
    "print(len(resblock.backbone))\n",
    "for i, conv_bn in enumerate(resblock.backbone[:-1], start=1):\n",
    "    print(i, conv_bn, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, bottleneck_channels: int, out_channels: int, stride: int):\n",
    "        \"\"\"\n",
    "        * `in_channels` is the number of channels in $x$\n",
    "        * `bottleneck_channels` is the number of channels for the $3 \\times 3$ convlution\n",
    "        * `out_channels` is the number of output channels\n",
    "        * `stride` is the stride length in the $3 \\times 3$ convolution operation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # First $1 \\times 1$ convolution layer, this maps to `bottleneck_channels`\n",
    "        self.conv1 = nn.Conv2d(in_channels, bottleneck_channels, kernel_size=1, stride=1)\n",
    "        # Batch normalization after the first convolution\n",
    "        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n",
    "        # First activation function (ReLU)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        # Second $3 \\times 3$ convolution layer\n",
    "        self.conv2 = nn.Conv2d(bottleneck_channels, bottleneck_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        # Batch normalization after the second convolution\n",
    "        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n",
    "        # Second activation function (ReLU)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        # Third $1 \\times 1$ convolution layer, this maps to `out_channels`.\n",
    "        self.conv3 = nn.Conv2d(bottleneck_channels, out_channels, kernel_size=1, stride=1)\n",
    "        # Batch normalization after the second convolution\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut connection should be a projection if the stride length is not $1$\n",
    "        # of if the number of channels change\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # Projection $W_s x$\n",
    "            self.shortcut = ShortcutProjection(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            # Identity $x$\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        # Second activation function (ReLU) (after adding the shortcut)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` is the input of shape `[batch_size, in_channels, height, width]`\n",
    "        \"\"\"\n",
    "        # Get the shortcut connection\n",
    "        shortcut = self.shortcut(x)\n",
    "        # First convolution and activation\n",
    "        x = self.act1(self.bn1(self.conv1(x)))\n",
    "        # Second convolution and activation\n",
    "        x = self.act2(self.bn2(self.conv2(x)))\n",
    "        # Third convolution\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        # Activation function after adding the shortcut\n",
    "        return self.act3(x + shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBase(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_blocks: List[int],\n",
    "                 n_channels: List[int],\n",
    "                 bottlenecks: Optional[List[int]] = None,\n",
    "                 img_channels: int = 3,\n",
    "                 first_kernel_size: int = 7\n",
    "                 ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        n_blocks is a list of of number of blocks for each feature map size.\n",
    "        n_channels is the number of channels for each feature map size.\n",
    "        bottlenecks is the number of channels the bottlenecks. If this is None , residual blocks are used.\n",
    "        img_channels is the number of channels in the input.\n",
    "        first_kernel_size is the kernel size of the initial convolution layer\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 5, 128, 128])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(64, 3, 128, 128)\n",
    "resblock(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mBottleneckResidualBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0min_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbottleneck_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mout_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Base class for all neural network modules.\n",
      "\n",
      "Your models should also subclass this class.\n",
      "\n",
      "Modules can also contain other Modules, allowing to nest them in\n",
      "a tree structure. You can assign the submodules as regular attributes::\n",
      "\n",
      "    import torch.nn as nn\n",
      "    import torch.nn.functional as F\n",
      "\n",
      "    class Model(nn.Module):\n",
      "        def __init__(self):\n",
      "            super(Model, self).__init__()\n",
      "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "\n",
      "        def forward(self, x):\n",
      "            x = F.relu(self.conv1(x))\n",
      "            return F.relu(self.conv2(x))\n",
      "\n",
      "Submodules assigned in this way will be registered, and will have their\n",
      "parameters converted too when you call :meth:`to`, etc.\n",
      "\n",
      ":ivar training: Boolean represents whether this module is in training or\n",
      "                evaluation mode.\n",
      ":vartype training: bool\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "* `in_channels` is the number of channels in $x$\n",
      "* `bottleneck_channels` is the number of channels for the $3    imes 3$ convlution\n",
      "* `out_channels` is the number of output channels\n",
      "* `stride` is the stride length in the $3       imes 3$ convolution operation.\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "BottleneckResidualBlock?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d75bf56f6470173b1ac7bd9da905d241387559ca422a37986719b723ab6f9d4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
